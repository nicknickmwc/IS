{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aace8cf-4db2-4787-a498-13fe1b58e57b",
   "metadata": {},
   "source": [
    "## Лабораторная работа №4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d253d94-d291-421f-b09a-882110919b36",
   "metadata": {},
   "source": [
    "## Распознавание рукописных символов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300c3e9c-b445-41a9-8205-07a8fab846e3",
   "metadata": {},
   "source": [
    "## Цель\n",
    "Реализовать классификацию черно-белых изображений рукописных цифр (28x28) по 10\n",
    "категориям (от 0 до 9)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc29f031-3d80-41d8-b993-e37dd04edae0",
   "metadata": {},
   "source": [
    "## Задачи\n",
    "1. Ознакомиться с представлением графических данных\n",
    "2. Ознакомиться с простейшим способом передачи графических данных нейронной сети\n",
    "3. Создать модель\n",
    "4.  Настроить параметры обучения\n",
    "5.  Написать функцию, позволяющая загружать изображение пользователи и классифицировать его"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c317c68-28d4-460a-ad5c-15d531a50d22",
   "metadata": {},
   "source": [
    "## Выполнение работы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652723c7-ee97-431a-bc19-b8bb25668b4c",
   "metadata": {},
   "source": [
    "Набор данных MNIST уже входит в состав Keras в форме набора из четырех массивов\n",
    "Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "969084d7-a62a-46ae-8f3c-6e147cdc054e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 10s 1us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels),(test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15397ae1-8217-4203-b158-07e040377636",
   "metadata": {},
   "source": [
    "Здесь train_images и train_labels — это тренировочный набор, то есть данные,\n",
    "необходимые для обучения. После обучения модель будет проверяться тестовым (или\n",
    "контрольным) набором, test_images и test_labels. Изображения хранятся в массивах\n",
    "Numpy, а метки — в массиве цифр от 0 до 9. Изображения и метки находятся в прямом\n",
    "соответствии, один к одному.\n",
    "Для проверки корректности загрузки достаточно сравнить тестовое изображение с его\n",
    "меткой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7054758-f57c-4d8c-a0ad-58b52e3a6424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(train_images[1],cmap=plt.cm.binary)\n",
    "plt.show()\n",
    "\n",
    "print(train_labels[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d787fa78-1a9b-4ad1-af0f-53dea419679f",
   "metadata": {},
   "source": [
    "Исходные изображения представлены в виде массивов чисел в интервале [0, 255]. Перед\n",
    "обучением их необходимо преобразовать так, чтобы все значения оказались в интервале\n",
    "[0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bd376dc-6e28-43c9-8794-3a69bc4b371a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a20258-6243-48f0-b025-f9867ba392b7",
   "metadata": {},
   "source": [
    "Также необходимо закодировать метки категорий. В данном случае прямое кодирование\n",
    "меток заключается в конструировании вектора с нулевыми элементами со значением 1 в\n",
    "элементе, индекс которого соответствует индексу метки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0772e032-2411-4201-92b4-4809ad828a3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee73b175-f79b-4114-b3bd-463941fbf81d",
   "metadata": {},
   "source": [
    "Теперь можно задать базовую архитектуру сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf90a9d8-b76f-4f99-afc7-2cbea2a60bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "model = Sequential()\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c25f11-713b-4695-8148-5f22e61db84e",
   "metadata": {},
   "source": [
    "Чтобы подготовить сеть к обучению, нужно настроить еще три параметра для этапа\n",
    "компиляции:\n",
    "1. функцию потерь, которая определяет, как сеть должна оценивать качество своей\n",
    "работы на обучающих данных и, соответственно, как корректировать ее в\n",
    "правильном направлении;\n",
    "2. оптимизатор — механизм, с помощью которого сеть будет обновлять себя, опираясь\n",
    "на наблюдаемые данные и функцию потерь;\n",
    "3. метрики для мониторинга на этапах обучения и тестирования — здесь нас будет\n",
    "интересовать только точность (доля правильно классифицированных\n",
    "изображений)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7be6406-7dbf-44c2-8b89-37e161094d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82753f07-5877-49d6-aef1-defe58dee93d",
   "metadata": {},
   "source": [
    "Теперь можно начинать обучение сети, для чего в случае использования библиотеки Keras\n",
    "достаточно вызвать метод fit сети — он пытается адаптировать (fit) модель под обучающие\n",
    "данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08ccb715-a533-4873-841f-6d8016ddb0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3112 - accuracy: 0.9131\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1349 - accuracy: 0.9617\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0916 - accuracy: 0.9736\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0682 - accuracy: 0.9798\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0526 - accuracy: 0.9844\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19e589cb6d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9eecf4-35d8-463a-8e57-1427d5aeaa2a",
   "metadata": {},
   "source": [
    "В процессе обучения отображаются две величины: потери сети на обучающих данных и\n",
    "точность сети на обучающих данных.\n",
    "Теперь проверим, как модель распознает контрольный набор:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f4ab063-aa9a-453c-bc68-76987b875870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0747 - accuracy: 0.9785\n",
      "test_acc: 0.9785000085830688\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671ddd4c-9e81-4b2f-96bb-fbf96d47919a",
   "metadata": {},
   "source": [
    "## Требования\n",
    "\n",
    "1. Найти архитектуру сети, при которой точность классификации будет не менее 95%\n",
    "2. Исследовать влияние различных оптимизаторов, а также их параметров, на процесс обучения\n",
    "3. Написать функцию, которая позволит загружать пользовательское изображение не из датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc934858-9a43-450e-a66b-3462a2f763bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1 Найдем архитектуру, при которой точность классификации будет не менее 95 процентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdfc36ef-d5db-4d40-a71f-5fd1e8f36e9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3800 - accuracy: 0.8959\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1765 - accuracy: 0.9495\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.1303 - accuracy: 0.9620\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 1s 1ms/step - loss: 0.1025 - accuracy: 0.9711\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0844 - accuracy: 0.9757\n",
      "313/313 [==============================] - 0s 933us/step - loss: 0.0994 - accuracy: 0.9693\n",
      "test_acc: 0.9692999720573425\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(100, activation='relu'))\n",
    "model2.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model2.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model2.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
    "\n",
    "test_loss, test_acc = model2.evaluate(test_images, test_labels)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05fedfdc-f2b3-4b78-96a5-ee27e10b4469",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3139 - accuracy: 0.9114\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1281 - accuracy: 0.9623\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0882 - accuracy: 0.9739\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0660 - accuracy: 0.9801\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0521 - accuracy: 0.9837\n",
      "313/313 [==============================] - 0s 965us/step - loss: 0.0820 - accuracy: 0.9739\n",
      "test_acc: 0.9739000201225281\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(128, activation='relu'))\n",
    "model3.add(Dense(128, activation='relu'))\n",
    "model3.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model3.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model3.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
    "\n",
    "test_loss, test_acc = model3.evaluate(test_images, test_labels)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d195322-bc2c-4f59-bd94-b02c2847211a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2168 - accuracy: 0.9337\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0945 - accuracy: 0.9701\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0670 - accuracy: 0.9790\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0513 - accuracy: 0.9832\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0408 - accuracy: 0.9868\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0794 - accuracy: 0.9773\n",
      "test_acc: 0.9772999882698059\n"
     ]
    }
   ],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(Flatten())\n",
    "model4.add(Dense(128, activation='relu'))\n",
    "model4.add(Dense(784, activation='relu'))\n",
    "model4.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model4.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model4.fit(train_images, train_labels, epochs=5, batch_size=32)\n",
    "\n",
    "test_loss, test_acc = model4.evaluate(test_images, test_labels)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d740c0c9-75fa-4a2a-9fe8-4f371ae8a1df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.1793 - accuracy: 0.9453\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 14s 8ms/step - loss: 0.0815 - accuracy: 0.9751\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 14s 8ms/step - loss: 0.0580 - accuracy: 0.9818\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 14s 8ms/step - loss: 0.0444 - accuracy: 0.9860\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 14s 8ms/step - loss: 0.0345 - accuracy: 0.9890\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.1046 - accuracy: 0.9736\n",
      "test_acc: 0.9735999703407288\n"
     ]
    }
   ],
   "source": [
    "model5 = Sequential()\n",
    "model5.add(Flatten())\n",
    "model5.add(Dense(784, activation='relu'))\n",
    "model5.add(Dense(1000, activation='relu'))\n",
    "model5.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model5.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model5.fit(train_images, train_labels, epochs=5, batch_size=32)\n",
    "\n",
    "test_loss, test_acc = model5.evaluate(test_images, test_labels)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605340ee-267e-461a-b251-dfe71f73fd06",
   "metadata": {},
   "source": [
    "## Исследуем различные оптимизаторы и их параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b56edb16-2c24-4ab8-972c-6ea8f59fc6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2089 - accuracy: 0.9360\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1096 - accuracy: 0.9671\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0857 - accuracy: 0.9745\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0780 - accuracy: 0.9769\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0680 - accuracy: 0.9794\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1482 - accuracy: 0.9715\n",
      "test_acc: 0.9714999794960022\n"
     ]
    }
   ],
   "source": [
    "model6 = Sequential()\n",
    "model6.add(Flatten())\n",
    "model6.add(Dense(256, activation='relu'))\n",
    "model6.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model6.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model6.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
    "\n",
    "test_loss, test_acc = model6.evaluate(test_images, test_labels)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25630337-07fa-422b-af6e-2dbc1fc128c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2898 - accuracy: 0.9180\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1281 - accuracy: 0.9628\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0880 - accuracy: 0.9742\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0662 - accuracy: 0.9804\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0520 - accuracy: 0.9847\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0777 - accuracy: 0.9770\n",
      "test_acc: 0.9769999980926514\n"
     ]
    }
   ],
   "source": [
    "model7 = Sequential()\n",
    "model7.add(Flatten())\n",
    "model7.add(Dense(256, activation='relu'))\n",
    "model7.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model7.compile(optimizer='RMSprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model7.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
    "\n",
    "test_loss, test_acc = model7.evaluate(test_images, test_labels)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f44b2b0-590a-4155-9250-02eb45a039b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 1.1067 - accuracy: 0.7484\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5293 - accuracy: 0.8682\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4263 - accuracy: 0.8870\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3793 - accuracy: 0.8964\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3505 - accuracy: 0.9036\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3254 - accuracy: 0.9099\n",
      "test_acc: 0.9099000096321106\n"
     ]
    }
   ],
   "source": [
    "model8 = Sequential()\n",
    "model8.add(Flatten())\n",
    "model8.add(Dense(256, activation='relu'))\n",
    "model8.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model8.compile(optimizer='SGD',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model8.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
    "\n",
    "test_loss, test_acc = model8.evaluate(test_images, test_labels)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d12e8c-f7ac-4f57-a8ac-c429d791f6ac",
   "metadata": {},
   "source": [
    "## Функция которая позволяет загружать пользовательские изображения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf494063-7e9c-4217-9333-8669b0feb6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "def img(name_img):\n",
    "    img_new = Image.open(name_img)\n",
    "    img_new = img_new.convert('L')\n",
    "    img_new = np.asarray(img_new)\n",
    "    img_new = img_new / 255.0\n",
    "    x = np.expand_dims(img_new, axis=0)\n",
    "    res = model4.predict(x)\n",
    "    print(res)\n",
    "    print(f\"Распознанная цифра: {np.argmax(res)}\")\n",
    "    plt.imshow(img_new, cmap=plt.cm.binary)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "163df37d-6085-4eaa-9387-48c0cf49da1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n",
      "[[1.4924008e-05 3.3602387e-02 6.3467227e-02 8.9983207e-01 2.7415899e-03\n",
      "  1.3592410e-04 4.9161309e-07 5.1934643e-05 7.5441982e-05 7.7980832e-05]]\n",
      "Распознанная цифра: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img('img1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "757da76c-9d97-4e8a-bfe0-836a879adae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n",
      "[[3.1650525e-05 3.7010029e-02 1.4638182e-04 1.8046210e-05 9.5575142e-01\n",
      "  4.1165515e-03 2.4940125e-03 2.2037231e-05 3.9546768e-04 1.4297854e-05]]\n",
      "Распознанная цифра: 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img('img2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075b0e24-d012-419d-a2a1-9173d72144d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eef95b89b117c20879678c0826dbc0e65d56cf8ab983c95002ee10fa11e0c03f"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
